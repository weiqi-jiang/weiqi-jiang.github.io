---
layout: post
title: 机器学习分类和回归模型汇总
category: ML
tags: Machine Learning
description: 汇总常见的分类和回归模型
---

<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
## Adaboost

基本思想在于上一级分类器分错的样本会被加权对待，加权后的全体样本再次被用来训练下一个基分类器，直到收敛，收敛条件一般为达到足够小的错误率或者达到最大迭代次数

训练步骤：

1. 给每个样本赋予相同的初始权重，假设有N个样本，那么每个样本的初始权重为1/N
2. 使用**全部样本**训练一个基分类器，对于分类正确的样本降低权重，分类错误的样本增加权重。权重更新后的样本用于训练下一个基分类器，基分类器的的value set ->{ +1. -1}
3. 最后进行模型融合的时候，增大低误差率的分类器的权重，降低高误差率低分类器的权重

一个基分类器的误差率$e_m$就是被该分类器分错样本的权重总和，值得注意的是，误差率是针对单个分类器的，不是之前已经训练过的级联分类器的。在每次训练基分类器的时候都以**最小化误差率**选择参数及阈值。即误差率为模型训练的损失函数，根据基分类器的误差率计算该基分类器在最后的复合模型中的重要程度。

$$
\alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}
$$

完成一次基分类器的训练之后需要更新样本权重，更新公式如下：

$$
D_{m+1} = (w_{m_1,1}, w_{m+1,2}... w_{m+1,i}...w_{m+1,N})\\
w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)), i = 1,2,3...N\\
w_{m+1,i} = \{_{\frac{w_{mi}}{Z_m}e^{-a_m}, G_m(x_i)=y_i}^{\frac{w_{mi}}{Z_m}e^{a_m}, G_m(x_i)\neq y_i}
$$

其中：

$Z_m$ 是规范化函数，归一化函数，作用是把所有权重的和规范化为1.

$G_m(x_i)$是第m个基分类器关于样本的分类结果

公式解析：

如果分类正确，$Y_i$和$G_m(x_i)$结果同号，$exp(-a_m)$由于 $a_m$一定为正，$exp(-a_m)$小于1， 那么分类正确的样本的权重就自然被压低，反正分类错误的样本的权重被提高, 最后的级联分类器由各个基分类器的线性组合完成

$$
G(x)= sign(f(x)) = sign(\sum_{m=1}^{M}a_mG_m(x))
$$

**前向分布算法：**

线性模型一般如下：

$$
f(x) = \sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$

其中, $b$代表基分类器,$\beta$示基分类器的权重, 整体模型的优化目标如下

$$
min\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i;\gamma_m))
$$

L代表损失函数，线性模型的整体训练目标是最小化整体的损失函数，即达到全局最优解；前向加法模型成为经验风险最小化的问题，即损失函数最小化的问题；简化问题的求解过程为用局部最优解逼近全局最优解，即每次只优化当前基分类器的损失函数，选择最优的参数组合使得当前单个基分类器的损失函数最小。

$$
min\sum_{i=1}^{N}L(y_i,\beta_mb(x_i;\gamma_m))
$$

在每次训练单个基分类器的时候，整体的优化目标如下， 由于上一级级联分类器的输出是固定的，所以只优化当前单个基分类器的参数即可，无数个局部最小化损失函数，构成了全局最小损失函数

$$
(\beta_m,\gamma_m) = argmin\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
$$

但是为什么前向分布算法就可以无限逼近全局最优解呢？为什么样本的权重更新函数是exp函数？ 证明如下：

\# to be completed

**在样本更新函数，以及损失函数为exp函数的时候，前向分布方法等到的最优解和直接解全局最优解的结果一样。**

**Reference**<br>[Adaboost算法详述 ](https://zhuanlan.zhihu.com/p/42915999)<br>[Adaboost 算法的原理与推导](https://blog.csdn.net/v_JULY_v/article/details/40718799)

## Decision Tree

二叉树的一些基本知识

- 高度：根节点高度为1，每一层高度加1
- 深度：根节点深度为0，每一层深度加1，可以理解为路径的个数

决策树的生成就是一个递归选择最优划分特征对训练集进行划分的过程

递归的结束条件

- 当前节点的所有sample属于同一个类别
- 当前节点sample数为零
- 当前可供划分的feature数为零

决策树的特征：

1. 过程容易理解，可视化能力强，直观
2. 应用范围广，分类回归都可以适用
3. 能够同时处理离散型和连续性的特征
4. 如果不进行剪枝操作，很容易过拟合
5. **学习一个最优的决策树是一个NPC问题，目前采用的是贪心算法，用多个局部最优去近似达到全局最优**。每次特征划分时，都是按照当前局部最优的划分特征进行划分，但是**局部最优的叠加效果不一定是全局最优结果**。
6. 决策树的输入不需规范化
7. 适合处理稠密特征，**不适合大规模稀疏性特征**

决策树生成算法大体优化路径 **ID3->C4.5->CART**

**ID3**

**支持多叉树，但是不能处理连续性变量**。采用"最大信息熵增益"作为优化目标，即当前特征的特定取值划分能达到最大信息熵增益，即以当前特征的取值划分数据，在之后的划分中，该特征不再起作用。确定划分特征之后，如果该特征有5个取值，那么该节点在划分之后产生5个child nodes，这也就是为什么ID3算法只能适用于离散型特征的原因；如果处理连续性特征，训练时，按照训练集中出现的所有可能取值划分，也可以完成训练，但是当做testing的时候，如果某一个特征中出现了训练时没有出现过得取值，那该样本应该被划分为哪一个分支就是一个问题了。“最大信息熵增益”公式如下：

$$
数据集D经验熵H(D) = -\sum^{K}_{k=1}\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}\\
特征A对数据集D的经验条件熵H(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)\\
计算信息增益g(D,A) =H(D)-H(D|A)
$$

公式含义解析： 数据集D的经验熵即是将K类样本的信息熵相加，此时是按照类别求累计信息熵； 公式（2）特征A对数据集D的经验条件熵的含义是：按照特征A取值划分之后，每一个取值下的样本计算经验熵的总和.最大信息熵增益要求选出一个指标，用该指标划分可以**使得样本的混乱程度降低的最多**。

ID3算法的特征（优缺点）

- **算法偏向于取值较多的特征**。如果一个特征的取值很多，每个取值对应的样本数很少，样本纯度高，倾向于使用该特征进行划分

- 没有考虑缺失值和连续性特征的问题

- 没有考虑过拟合问题

**C4.5**

在最大信息熵增益的基础上进行改进，抑制选择取值多特征的倾向，提出“最大信息熵增益比”的概念，“信息熵增益比”的公式如下

$$
SplitInformation(D,A) = -\frac{|D_i|}{|D|}log\frac{|D_i|}{|D|}\\
GainRatio(D,A) = \frac{g(D,A)}{SplitInformation(D,A)}
$$

公式解析： spilt information 中Di 的含义是特征A某个特定取值的样本集，如果特征A取值多，样本量少，样本纯度高，split information的值高，那么信息增益比自然降低，**增加了对连续特征的支持**，对于连续性特征的处理是连续性特征二元离散化，对特征取值排序，以连续两个值中间值作为新的候选取值，假如训练集中有N个取值，那么就有N-1个候选取值，候选取值的含义是小于或者大于该值，并不仅是等于该值；训练时，遍历所有特征，内部遍历特征下的所有可能候选取值，计算信息增益比。**选择信息增益比最大值对应的特征**(特别注意不是按照当前取值二元划分，而是按照取值所在属性，按照N-1个候选取值多元划分，所以**C4.5的树是多叉树**)

缺失值处理

- 丢弃样本

- 赋予特征最常见的值

- 按照当前样本集中取值出现的概率的取值。例如A的概率0.6，B的0.4.那么缺失值有60%被分配为A，40%分配为B

**CART(Classification And Regression Tree)**

CART树是二叉树，且每个非叶子节点都有两个子节点，所以cart树的非叶子节点的数量比叶子节点数量少1。CART树即可以做回归也可以做分类，ID3,C4.5只能做分类。CART做分类问题的时候，由于ID3 C4.5涉及到大量的log计算，而且C4.5还涉及到排序操作(因为支持了连续特征，需要找到两个相邻取值的中间值)，运算复杂度高。CART算法对应改进采用gini系数，**gini系数表征的是样本不纯度，故gini系数越低越好**，与ID3,C4.5的metric越高越好不同。Gini系数的公式如下，图的横坐标表示概率p，二分类问题下0.5\*熵和gini系数很接近

![WeChat Screenshot_20190621185537](/assets/img/ML/one-stop-machine-learning/gini.png)

公式解析：公式1 为（1 - 样本集中属于某个类别的概率平方和；）即样本集的gini指数；公式2 如果按照特征A 分割，两边的gini指数和，Di/D表示分配到两边的样本集占总样本集的比例

gini系数和熵的区别：

- gini系数不需要复杂的log运算

- gini偏向于连续性特征，理由同ID3偏向取值多的类似，熵偏向于离散型特征

- 熵和gini系数都是表征混乱程度，在x=1时gini系数和熵近似相等

![WeChat Screenshot_20190621200417](/assets/img/ML/one-stop-machine-learning/gini-entropy.png)

**分类树+categorical feature**：类别特征无需额外处理，涉及到的点主要是类别特征如何排序，**按照正样本数的数量升序排序**，例如类别特征取值有A,B,C三种，取值对应的正样本数有，10,5,8个，那么排序就是B->C->A,首先把B放左子树，其他放右子树，计算gini系数的减少量，以此类推，找到最大减少量对应的分割

**分类树+numerical feature**：对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数，而且CART树只进行2分，C4.5是多分。具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则CART算法取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：Ti=(ai+ai+1)/2。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为at,则小于at的值为类别1，大于at的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则**该属性后面还可以参与子节点的产生选择过程**。

CART名字叫做classification and regression tree 那么肯定就不光有classification的部分，也要有regression的功能，分类时使用gini系数衡量样本不纯度，回归时就用**均方误差**代替

$$
MSE(D) = \frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2
$$

其中y hat表示样本的预测值，y表示样本值,D表示节点上的样本集，一般情况下某个样本的预测值等于节点上所有样本值的平均，那么一个节点上的均方误差可以写成

$$
MSE(D) = \frac{1}{N}\sum_{i=1}^{N}(y_i-\frac{\sum_{i=1}^{N}y_i}{N})^2 \\
 \qquad=\frac{1}{N}\sum_{i=1}^{N}[y_t^2-\frac{2y_i\sum_{i=1}^{N}y_i}{N}+\frac{(\sum_{i=1}^{N} y_i)^2}{N^2}]\\
 =\frac{1}{N}[\sum_{i=1}^{N}y_t^2-\frac{2(\sum_{i=1}^{N}y_i)^2}{N}+\frac{(\sum_{i=1}^{N} y_i)^2}{N}]\\
 =\frac{1}{N}[\sum_{i=1}^{N}y_i^2-\frac{(\sum_{i=1}^{N} y_i)^2}{N}]
$$

对于某个特征A，把集合D划分为m个部分，划分后的均方误差如下，一般m为2

$$
MSE(D,A) = \sum_{i=1}^{m}\frac{N_i}{N}MSE(D_i) \\
误差减少量=MSE(D) - MSE(A,D)
$$

寻求的就是最大化误差减少量， 我们把MSE(D)的值带入上式，且假设树为二叉树

$$
\triangle MSE(D,A^{(\beta)}) = MSE(D) - MSE(D,A) \\
=\frac{1}{N}[\sum_{i=1}^{N}y_i^2-\frac{(\sum_{i=1}^{N} y_i)^2}{N}] - \frac{R}{N}(\frac{1}{R}[\sum_{i=1}^{R}r_i^2-\frac{(\sum_{i=1}^{R} r_i)^2}{R}]) - \frac{L}{N}(\frac{1}{L}[\sum_{i=1}^{L}l_i^2-\frac{(\sum_{i=1}^{L} l_i)^2}{L}])\\
=-\frac{(\sum_{i=1}^{N}y_i)^2}{N^2} + \frac{1}{N}[\frac{(\sum_{i=1}^{R}r_i)^2}{R} + \frac{(\sum_{i=1}^{L}l_i)^2}{L}]
$$

可以看出误差的减少量**完全取决于上式中的后半部分**，一种极端情况，所有样本值一样，那么误差减少量为0，也就是划分没有带来收益，从直观上来看也确实符合。另一种极端情况，样本值有0和100两种取值，把所有0值放在左边，所有100放在右边，很容易理解这种情况下误差减少量是所有情况下最大的，同时树模型的效果也是最好的。

**回归树+categorical feature：**按照每个特征属性样本值的平均值来升序排序，带入上面的式子中，计算最大值

**回归树+numerical feature：**与分类树相同，按照特征取值的大小升序排序，计算均方误差减少量的最大值

**overfitting**

针对过拟合问题可以通过剪枝来控制，在不剪枝之前，完全展开的决策树每个叶子节点都代表0/1 ，剪枝之后，叶子节点上划分的可能同时具有0和1样本，此时做reference的时候，如果样本被分为该节点上，则那类样本多，就认为新输入被分为哪一类。**预剪枝**在节点划分前提前进行评估，若当前划分不能带来泛化性的提升，则停止划分，将该节点标记为叶子节点（验证方法为cross validation）**后剪枝**先生成一颗完整的树，自下而上的评估非叶子节点，若剪枝能带来泛化性的提升，则进行剪枝操作

![081120312282_0WeChat](/assets/img/ML/one-stop-machine-learning/tree-cutting.png)

剪枝过程就是最小化决策树整体的损失函数，损失函数如下，等于**每一个叶子节点上样本的经验熵\*样本个数求和+叶子节点个数**，叶子越多，叶子节点上的经验熵越小，整体损失函数是变大还是变少，受两个方面的影响，就限制了决策树无限增殖的趋势

![WeChat Screenshot_20190811204303](/assets/img/ML/one-stop-machine-learning/tree-cutting-loss.png)

**Reference**<br>[数据挖掘十大经典算法--CART: 分类与回归树](https://wizardforcel.gitbooks.io/dm-algo-top10/content/cart.html)<br>[决策树模型 ID3/C4.5/CART算法比较](https://www.cnblogs.com/wxquare/p/5379970.html)<br>[决策树方法小结](https://blog.csdn.net/yujianmin1990/article/details/47406037)

**Code Reference**<br>[机器学习之分类回归树(python实现CART)](https://www.jianshu.com/p/8863641a30b1)<br>[决策树思想与Python实现：CART](https://blog.csdn.net/u013719339/article/details/84502265)<br>[Python实现C4.5(信息增益率)](https://www.cnblogs.com/wsine/p/5180315.html)

## Random Forest

随机森林中的每一颗树都是CART树，每一颗树**不进行剪枝**

**Random Forest的随机性**

randomly choose feature candidate reduce the correlation between trees

- 每个树有自己的样本集(有放回抽样得到, Bagging)

- 每个节点的在进行特征划分的时候，是从总特征集中随机抽取一个**子特征集**，在这个子特征集中选择最优划分特征和划分值。

**优点**

- 高准确度

- 随机性的引入不容易过拟合，泛化能力强

- 处理高纬度输入，并且无需做特征选择

- 同时支持离散和连续性特征

- 训练速度快，可以得到特征importance

- 并行化计算

**缺点**

- 噪声较大的分类回归问题上过拟合（为什么？）

- 树越多才稳定，但是树越多，模型越大

- 不善于处理不平衡数据集（为什么？可能是因为每棵树的训练样本是随机抽取的子样本集，如果不平衡， 那么完全有可能没有少数样本，根本学不到少数样本的特征）

## GBDT

采用的是加法模型和前向分布算法，与Adaboost类似

优化函数：

$$
\hat{\theta}_m = argmin\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i) + T(x_i;\theta_m))
$$
其中L代表损失函数，$f_{m-1}(x)$ 为m-1个独立的分类器的输出, T为当前分类器的输出。损失函数可以不同，回归问题一般采用**均方误差函数**，分类问题一般采用**交叉熵损失函数**。**需要特别注意的是，在每一棵树生成的时候，无论是分类还是回归，都是按照两个子节点均方误差最小的标准**。**GBDT中全部都是回归树**（因为只有回归树累加才有意义，分类树累加无意义）每棵树拟合的都是上一个树的损失函数的负梯度方向。

对于回归问题，一般采用**均方误差**作为损失函数，则**负梯度方向就等于残差，这里的残差具体是所有样本集的残差平均值**，下一级回归树所有训练样本集的label变为残差值

![imJx56v](/assets/img/ML/one-stop-machine-learning/gbdt-algo.png)

树与树之间是拟合上一颗树输出的残差，那么树内到底是用什么损失函数进来进行特征划分的，其实也是**平方误差损失函数**(注意不是均方误差，不用平均), 上文中也说到了，在平方误差小于某个数的时候结束，为了防止过拟合，会进行剪枝操作，具体的训练过程举例

![9RWud8o](/assets/img/ML/one-stop-machine-learning/gbdt-sample.png)
![NqRfj0q](/assets/img/ML/one-stop-machine-learning/gbdt-sample2.png)

但是对于分类问题，情况就稍微复杂一点，以二分类为例，多分类原理其实类似，因为GBDT里所有树都是回归树，输出值域是正无穷到负无穷，不是一个0-1之间的值。所以单独的一棵树的输出需要经过sigmoid函数映射成0-1之间的概率值，表示为$\hat{y} = sigmoid(\alpha)$, 其中$\alpha$ 是树的原始输出，但是这只是解决了输出映射的问题，分类问题的损失函数是什么呢？分类问题最常见的损失函数就是交叉熵，表示为

$$
cross\_entropy = -[ylog\hat{y} + (1-y)log(1-\hat{y})]\\
\hat{y} = sigmoid(\alpha)
$$

其中交叉熵和sigmoid的梯度分别是,对$\hat{y}求导$，要优化什么就对什么求导，这里要优化GBDT的输出，也就是对$\hat{y}$求导

$$
\acute{cross\_entropy} = \frac{y}{\hat{y}} +\frac{y-1}{1-\hat{y}} =\frac{\hat{y}-y}{\hat{y}(1-\hat{y})}
$$

$$
\acute{\hat{y}} = \hat{y}*(1-\hat{y})
$$

这样我们对损失函数求导之后正好结果是$y-\hat{y}$,即残差，和回归问题形式吻合。

**GBDT的特点**

树的个数不能太多，后面的子树相当于在学习细节规律，如果过多，则容易造成过拟合

**GBDT和RF的适用性**

- GBDT 对异常值很敏感，因为异常值严重影响了均方误差 ，RF不敏感
- RF减小模型variance，GBDT减小模型bias

**Reference**<br>[GBDT算法用于分类问题](https://zhuanlan.zhihu.com/p/46445201)

## XGBoost

### Theory

XGBoost 和 GBDT 在结构上几乎一样，而且都采用additive training 方法, 加法模型损失函数有一个统一的形式
$$
\mathcal{L}^{(t)} = \sum_{i=1}^{n}l(y_i, \hat{y}_i^{(t-1)}+f_t(x_i)) + \Omega(f_t) \qquad (1)
$$

其中L(x,y)为任意损失函数, Ω(x)为惩罚项，f(x)为当前回归器的输出，如果损失函数为常用的平方误差函数，则目标函数如下：

$$
\mathcal{L}^{(t)} = \sum_{i=1}^{n}(y_i-(\hat{y}_i^{(t-1)}+f_t(x_i))^2 + \Omega(f_t) \\ =\sum_{i=1}^{n}[(y_i-\hat{y}_i^{(t-1)})^2+2(\hat{y}_i^{(t-1)}-y_i)f_t+f_t^2]  \qquad (2)
$$

对该目标函数求梯度

$$
\frac{\partial \mathcal{L}^{(t)}}{\partial \hat{y}_i^{(t-1)}} = 2\sum_{i=1}^{n}(\hat{y}_i^{(t-1)}-y_i)+constant \qquad (3)
$$

上式可以很明显的看出，当新加入的基分类器的输出正好是残差的时候，总损失函数最小，梯度为0，这个很好理解，我们要做的就是让基分类器的输出更可能的接近残差。那**如果损失函数不是平方误差函数呢**？因为平方误差函数在回归问题中经常被使用，基于它的理论推导十分的成熟，有没有可能使用不同的损失函数，但是共用一套理论推导，当然有可能！把式1泰勒二阶展开

$$
\mathcal{L}^{(t)} \simeq\sum_{i=1}^{n}[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t(x_i)^2]+\Omega(f_t)+constant
$$

对任意损失函数进行泰勒二阶展开，**为什么要二阶展开？**1是因为二阶展开往往已经对原有函数有足够高的近似程度，2是因为二阶展开具有最高二阶项，和平方误差函数类似。

- Xgboost官网上有说，当目标函数是MSE时，展开是一阶项（残差）+二阶项的形式（官网说这是一个nice form），而其他目标函数，如logloss的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把MSE推导的那套直接复用到其他自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。这是从**为什么会想到引入泰勒二阶**的角度来说的
- 二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法里已经证实了。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。这是从二阶导本身的性质，也就是**为什么要用泰勒二阶展开**的角度来说的

其中要注意的事$l(y_i,\hat{y}_i^{(t-1)})$是上一级分类器的损失函数，是一个固定值，省略掉常数项，损失函数有一个统一的形式

$$
\mathcal{L}^{(t)} =\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t(x_i)^2]+\Omega(ft)
$$

后续的理论推导都是基于上式，其中g,h分别是损失函数的一阶导和二阶导，到了只一步就可以看出XGBoost的厉害之处，上式中**包含了损失函数的一阶导和二阶导形式，但是并没有指出是哪一个具体的损失函数，也就是说任意一阶二阶可导的损失都可以适用于下述的所有推导, 使用不同的损失函数，就可以使XGBoost模型适用于不同的任务，回归，分类，排序。**

在进行后续推导之前，先要定义XGBoost的惩罚项，才方便合并表达式，方便后续推导。**一般一棵树的复杂度由这一颗树的叶子节点个数，以及叶子节点输出值的L2范数组成**，L2范数的引入使得每一级分类器的输出不宜过大，例如三级基分类器要拟合100，第一种情况是90 -> 9 -> 1，第二种情况是40 -> 40 -> 20 虽然最后结果一样，但是更倾向于选择后者。因为第一个分类器直接就90，容易过拟合，而且有一种观点是觉得每一个分类器只会学到真相的一部分，而不是绝大多数真相，衍生出shrinkage，为后续分类器留出学习空间。

![16](/assets/img/ML/one-stop-machine-learning/xgboost-tree-complexity.png)

**求解目标函数**

$$
\mathcal{L}^{(t)} =\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t(x_i)^2]+\Omega(ft) \\
=\sum_{i=1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2]+\gamma T+\frac{1}{2}\lambda\sum^{T}_{j=1}w_j^2\\
=\sum_{j=1}^{T}[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda )w_j^2]+\gamma T \\
=\sum_{j=1}^{T}[G_iw_j+\frac{1}{2}(H_i+\lambda)w_j^2] + \gamma T
$$
公式解析

- **w函数负责将叶子节点映射到它对应的输出上，q把输入映射到它应该在的叶子节点上**
- *I* 表示在某一叶子节点上所以样本集
- 第二行前半部分是所有训练样本的在它对应的叶子节点上的一阶二阶输出，注意累计i个样本，即训练样本，后半部分累加j个叶子节点的权重值。
- 理解第三行必须明白，所有叶子节点上的所有样本的个数和即为总训练样本数。所以所有训练样本的一阶二阶权重和就等于所有叶子节点上样本一阶二阶权重和，而且同一个叶子节点上的训练样本，输出值相同

很明显，在树结构固定的情况下，损失函数只和叶子结点的输出有关，对w求导等于0，然后将w导数等于0时的值带入目标函数得

$$
w_j^* = -\frac{G_j}{H_j+\lambda} \\
\mathcal{L}^{(t)}= -\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\lambda}+\gamma T
$$
此时的损失函数表示当前树的结构不变的情况下，仅仅改变叶子节点的权重，最极限的情况下的目标函数最小值，那么此时的**叶子结点输出值是最优的**，称为**Structure Score**

![21](/assets/img/ML/one-stop-machine-learning/xgboost-tree-complexity-loss5.png)

### Train

- 枚举不同树的结构，使用上述obj作为衡量标准寻找一个最优结构的树
- 迭代进行上一步直到满足终止条件

看似简单，但是里面涉及到一个问题，如何衡量特征划分的好坏，如何确定树的结构。决策树都采用贪心算法进行训练，每一次尝试对一个已有的叶子节点进行分割，分割的衡量增益是下图，**和GBDT不一样体现在三个点，损失函数不是平方误差，新增了惩罚项，叶子节点的输出不是样本值均值，而是最优化出来的**。

![22](/assets/img/ML/one-stop-machine-learning/xgboost-tree-gain.png)

有一个点值得注意，这个Gain表示增益当然是越大越好，可是obj应该是左子树+右子树应该小于不分割的情况啊。这是因为obj前面有负号，没有负号的Gain当然方向是反的，越大越好。**计算中不需要每次分割重新计算一遍G 和H**，因为这个变量只和上一级的模型损失函数有关，不管怎么分割每个样本该数值不变，所有只需要扫描一遍计算即可重复使用。但是和一般的决策树不同的是，XGBoost的训练停止条件不是无分割样本，所有样本属于同一种类，无可分割特征，**因为XGBoost加入了叶子节点复杂度惩罚项，如果分割带来的增益减去惩罚项之后增益不够大，甚至负增益那么也就不进行分割**

### Details

**缺失值处理**

xgboost在节点分列时不考虑缺失值的数值，缺失值会被分别放到左右子树中计算损失，选择更优的一种划分，如果训练时没有缺失，预测时出现缺失，默认划分到右子树。

**过拟合** ： shrinkage，正则项

**分裂算法/并行化**

分裂算法有两种，一种是**精确的贪心算法**(常用于单机计算)，另一种是**基于权重直方图的近似算法**(常用于分布式计算)；精确的贪心算法时间复杂度$O(nm+mlog(m))$ n表示n个特征，m表示m个样本,计算所有样本当前损失函数的一阶导和二阶导，排序特征取值，遍历所有可行切分点，找到收益最大切分点。

![exact greedy](/assets/img/ML/one-stop-machine-learning/exact-greedy.jpg)

重点讲解近似算法，近似算基于权重直方图，把排序好的特征划分为若干个特征子集，即特征离散化，只有在buckets边界点中筛选分裂取值，大大的减少了取值点，进而大幅减少计算量

![](/assets/img/ML/one-stop-machine-learning/approximate.png)

根据什么时候用权重直方图特征离散化，分成**全局选择和局部选择**。全局选择是per tree，在建树时依据所有样本对每维特征进行离散化，建立buckets，过程中重复利用buckets。局部选择是per split，在分裂时，基于当前节点上的样本计算buckets，在进行判断。很显然，局部选择的计算量明显大于全局选择， 但是精度也要更加接近Exact Greedy

那具体是怎么用权重直方图的方法来划分的，这个离散化的点是怎么确定的呢？首先对特征取值排序，对每个取值$z$计算rank值,**rank 值表示取值小于$z$的样本的二阶导之和占所有样本总二阶导之和的比例**。
$$
r_k(z) = \frac{1}{\sum_{(x,h)\in D_k h}}\sum_{(x,h) \in D_{k,x<z}}h
$$
其中$h$ 表示损失函数二阶导，$x$表示某个样本，$k$表示样本的第$k$个特征。 人为设定一个$\epsilon$ , 满足
$$
|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \epsilon \\
s_{k,1} = min_i(x_{ki}) \\
s_{k,l} = max_i(x_{ki})
$$


**xgboost 的特征重要性**

- *importance_type=*weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数。
- *importance_type=*gain，特征重要性使用特征在作为划分属性时loss平均的降低量。
- *importance_type=*cover，特征重要性使用特征在作为划分属性时对样本的覆盖度（就是有多少样本是通过这个特征划分开的）

### Features

- “模块化”灵活性，GBDT以CART作为基分类器，XGBoost除此之外还支持线性分类器，此时XGBoost相当于带L1,L2的线性分类器。
- 传统GBDT只利用了一阶导数信息，使用常用的平方误差损失函数，一阶导就是残差，XGBoost利用泰勒展开，使用了二阶导，实现虽然形式上和平方误差函数一致，但是更加模块化，只要函数可一阶二阶导，都可以喂进一个理论推导框架
- 添加正则项，降低模型的variance，使得模型更加简单，避免过拟合
- 和GBDT一样有shrinkage缩减操作
- XGBoost的剪枝操作是后剪枝，先训练到最大深度，然后往前剪枝。后剪枝的好处在于不会因为一个负增益就放弃了后面可能的更大的正向增益
- 可以处理缺失值
- xgboost的并行化是特征粒度上的并行，把数据提前排序保存为block结构，在确实划分特征和划分值的时候，重复使用结构，可以并行计算

**Reference**<br>[集成学习系列(3)-XGBOOST](https://zhuanlan.zhihu.com/p/61842123)

## SVM

**SVM 是一种结构风险最小化模型**

为了理解SVM 先来了解线性模型

线性函数： f(x) = w*x + b （在空间中为hyper plane）

- 式中的x不是二维坐标系下的x轴，而是坐标向量，例如二维坐标为（x，y） = （1,2）时，x为[2,1]因为要转置和w进行点乘
- g(x) 的输出是一个连续值，在进行分类问题是，需要sign函数将连续值规范到0,1两个类别
- g(x) 在空间中是以w为法向量的一组无限个超平面，其中g(x) =0 只是其中的一个超平面，g(x) =0 这个平面称为分类面
- 将g(x)>0 的点归为1类别，g(x)<0归为-1类别，如果g（x）=0 拒绝判断

**重点来了！**

实际中，满足条件的分类面可以不止一个，讲满足条件的一个分类面稍微“倾斜”一点，往往也可以满足要求，那么这两个同时都满足要求的分类面哪一个更好呢？这便是SVM和普通线性分类器的区别所在,引入margin的概念，为了区别多个可行解，确定一个解为最优解，SVM采用最大化Margin的原则，magin主要有两种，一种是函数间隔，一种是几何间隔，函数间隔(Functional Margin)

$$
func\_margin = y(w^Tx+b) = |w^Tx+b|
$$

在二分类问题下如果$w^T+b >0$样本会被分为正类+1，否则判定为-1，所以上式恒大于0;所有样本中函数间隔的最小值便是**数据集T关于分类面的函数间隔，**此种定义的弊端在于如果同时扩大W 和 b，在不改变超平面的情况下函数间隔扩大两倍，于是引入**几何间隔(Geometrical Margin)**的概念. 

![geometric_margin](/assets/img/ML/one-stop-machine-learning/geometric_margin.png)

假设一个超平面经过x点且和f(x) =0 平行， x在f(x)=0上的投影为x0，几何知识可以得到 

$$
x = x_0+\gamma\frac{w}{|w|}\\
w^T*w = ||w||^2\\
w^Tx_0+b = 0
$$

第一个式子两边同乘$w^T$,同加$b$,后式带入前式可以得到几何间隔的表达式即可得到几何间隔（geometrical margin）

$$
\gamma = \frac{w^Tx+b}{||w||} = \frac{f(x)}{||w||} \\ 
\tilde{\gamma} = y\gamma = \frac{\tilde{\gamma}}{||w||}
$$

由上两种距离的可以看出，函数间隔，用函数的值表示间隔，这是一个人工定义的量，几何间隔是空间中的真实距离,即为图中gap的一半, 当分类面离数据点的“间隔”越大，分类的置信度越大，这个很好理解。分类面是两个类别的分割面，越远离这个分割面，越有confidence相信这个结果的正确

![svm-gap](/assets/img/ML/one-stop-machine-learning/svm-gap.jpg)

**求解**

设$\tilde{\gamma}$固定为1 ,即设样本集中离f(x)=0的分离面的距离为1，该假设不影响优化结果（为什么不影响）

优化目标最大化下式

$$
object\quad\tilde{\gamma} = \frac{1}{||w||}\\
s.t. \quad y_i(w^Tx_i+b)\geq1
$$

上述优化问题等价于下式

$$
min\frac{1}{2}||w||^2\quad s.t.\quad y_i(w^Tx_i+b)\geq1,i=1,...,n
$$
优化函数为二次函数，约束条件为线性条件，所以是一个凸二次规划问题。

并且可以利用lagrange duality 转换为对偶问题求解，对偶问题（dual problem）达到最优解的时候，原问题也同时达到最优解。

对偶问题的好处在于

- 更容易求解
- 自然的引入核函数。

利用拉格朗日对偶性以及拉格朗日乘子法可以将原问题转换为如下对偶问题，拉格朗日对偶性就是对每一个约束条件加上一个拉格朗日乘子$\alpha$，定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出我们的问题）,带约束条件的凸二次优化问题，利用拉格朗日乘子法转换如下

$$
L(w,b,a) = \frac{1}{2}||w||^2-\sum_{i=1}^{n}a_i(y_i(w^Tx_i+b)-1)
$$
如果有任意一个样本点满足$y_i(w^Tx_i+b)<1$， 在a无限大的情况下 ， 目标函数就会趋近于无限小。所以硬性要求所有的样本点的最小距离大于等于1.当且仅当所有样本的约束条件得到满足，即所有样本的距离都为1时，θ 等于原优化问题 1/2*（\|\|w\|\|）^2, 最小化该值；为了使所有约束条件满足：

1. **所有support vector 的拉格朗日乘子可以不为零，因为y(wx+b)-1对于支撑向量来说等于0**
2. **所有非支持向量的拉格朗日乘子a为0**

![lag-alpha](/assets/img/ML/one-stop-machine-learning/lag-alpha.jpg)

所以最终的优化目标函数为，且该问题和原问题等价：

![svm-equ](/assets/img/ML/one-stop-machine-learning/svm-equ.jpg)

等价问题的对偶问题为：

该问题和原问题也是等价，但解答更方便，需要3步

1. 先L(w,b,a)关于w和b最小化
2. 然后对a 求极大
3. 求解拉格朗日乘子

![1351142316_5141](/assets/img/ML/one-stop-machine-learning/lag-mul-solution.jpg)

**求解对偶问题**

第一步：

固定a，L对W和b最小化，，对w和b求偏导，并将两个偏导打入之前的L

![lag-dual1](/assets/img/ML/one-stop-machine-learning/lag-dual1.jpg)

将偏导带入到原来的L中得到：

![lag-dual2](/assets/img/ML/one-stop-machine-learning/lag-dual2.jpg)
![lag-dual43](/assets/img/ML/one-stop-machine-learning/lag-dual43.jpg)

第二步，对a求极大；公式中只有a一个变量，只要解出a，带入右边的公式就可以解除w和b

![lag-dual5](/assets/img/ML/one-stop-machine-learning/lag-dual5.jpg)
![lag-dual6](/assets/img/ML/one-stop-machine-learning/lag-dual6.jpg)
![lag-dual62](/assets/img/ML/one-stop-machine-learning/lag-dual6.png)

第三步：

SMO算法求解拉格朗日乘子，带入上面的公式得到w和b

把w带入分类面函数得到右边的形式，可以看出新样本点的输出值和训练样本的内积有关，其实只有支持向量x对应的拉格朗日乘子a有值，其他a都为0，所以计算量也不算太大

![lag-dual7](/assets/img/ML/one-stop-machine-learning/lag-dual7.jpg)

**核函数（kernal）**

使得SVM具有非线性，在线性不可分的情况下，通过核函数将所有样本映射到高维空间，在高位空间中构造出分类面，从而将线性不可分样本转换为线性可分

如果不引入核函数，构造非线性分类器的方法就是引入一个非线性映射

原分类函数：

![kernal1](/assets/img/ML/one-stop-machine-learning/kernel1.jpg)

原函数对偶形式

![kernal2](/assets/img/ML/one-stop-machine-learning/kernal2.jpg)

此时如果有一个新的映射K(x1,x2) 把<Φ(xi), Φ(x)> 映射到内积特征空间，这个函数K 就是核函数

![kernal3](/assets/img/ML/one-stop-machine-learning/kernal3.jpg)

**核函数举例**

![kernal4](/assets/img/ML/one-stop-machine-learning/kernal4.png)

假设两类样本集如左图，简单看出分类面需要是一个圆形才能将两类样本分开，样本是线性不可分的。由圆的标准方程可以得知分类面的所在方程如下

![kernal5](/assets/img/ML/one-stop-machine-learning/kernal5.jpg)

如果把二维空间上升为5维空间，五个维度分别为[X1,  X1^2,  X2,  X2^2,  X1X2], 对应的权重w 分别为[a1, a2, a3, a4, a5], 偏置b为 a6。现在原来的非线性可分的样本，在五维空间下就变成线性可分的了， g(x) = w*x + b

此时观察原函数的对偶形式(如上)，假设两个向量 x1 = (n1,n2)T, x2 = (m1,m2)T, 经过Φ(x)映射之后x1,x2变为[n1, n1^2, n2,  n2^2, n1n2], [m1, m1^2, m2,  m2^2, m1m2].

经过映射后的特征空间，内积的结果：

![kernal6](/assets/img/ML/one-stop-machine-learning/kernal6.jpg)

假设我们不进行高维映射，仅仅在原来的特征空间x1 , x2 进行变换； 观察可以得到结果和高维映射的结果很相似，但是有些许不同，这些不同可以通过缩放维度达到完全相同，如右图，缩放结果是不影响可分性的。

![kernal7](/assets/img/ML/one-stop-machine-learning/kernal7.jpg)
![kernal8](/assets/img/ML/one-stop-machine-learning/kernal8.jpg)

这两个结果看着相同，但是思想完全不同：

- 第一个是把原特征空间映射到高维空间，然后根据内积公式进行计算
- 第二个是**直接在原来的低维空间进行计算，且映射后的结果是隐式的**

此时 核函数就是:

![kernal9](/assets/img/ML/one-stop-machine-learning/kernal9.jpg)

核函数的作用就是简化了映射空间中内积计算，这个效果是很明显的，上述例子仅仅对二维空间的一阶二阶的所有组合做映射，如果原始空间为3维，所有一阶二阶三阶的所有组合有19种，那么如果原始空间的维度再往上，高维空间的维度会呈指数级爆炸，**使用核函数避开了高维空间中的计算**，并且结果等价

原问题的对偶问题

以及使用核函数后的对偶问题

![kernal10](/assets/img/ML/one-stop-machine-learning/kernal10.jpg)
![kernal11](/assets/img/ML/one-stop-machine-learning/kernal11.jpg)

σ例子中核函数很好想出来，但是实际情况下，核函数的具体形式是很难构造出来的。所以实际使用中，常常是从常用的核函数类型中选择：

- 多项式核函数 k = (<x, xi> + R)^d ， 其中两个参数<R, d>
- 高斯核函数 k = exp(- ||x1-x2||^2/(2*σ^2)), 高斯核函数可以把原始空间映射到无穷维。通过调整参数σ 高斯核函数使用十分灵活。

核函数的选取规则：

1. 如果Feature的数量很大，跟样本数量差不多，且线性可分的时候这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，且线性不可分的时候选用SVM+Gaussian Kernel
3. 样本数量非常多选择线性核（避免造成庞大的计算量）

4. 当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数

5. 当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数

**松弛变量**

有的时候样本集对外显示线性不可分，但是如果允许若干个样本分类错误的话，往往这些分类错误的样本本身就是outlier，没有实际训练意义，样本可以转换为线性可分，或者说是近似线性可分。

![Optimal-Hyper-Plane-2](/assets/img/ML/one-stop-machine-learning/Optimal-Hyper-Plane-2.png)

图中蓝色的点就是分类错误的点，黑色线段就是分类错误带来的惩罚。

“硬间隔”分类下我们对于样本点集合距离的要求

![hard-margin](/assets/img/ML/one-stop-machine-learning/hard-margin.gif)

“软间隔”分类下引入松弛变量之后对样本点距离的要求，允许一些变量的距离小于1. ζ 即为松弛变量。可见如果松弛变量无穷大的话，所有样本点都可以满足要求，但是这样分类就变的无意义了。**使用松弛变量意味这我们放弃某些点的精确分类,**这样带来的好处也很明显，**不必使得分类面往outlier方向迁移，也可能获得更大的分类间隔**。

![soft-margin](/assets/img/ML/one-stop-machine-learning/soft-margin.gif)

可是怎么去衡量松弛变量带来的好处和坏处呢，即ζ 不能无限大啊。正则项可以选择**ζ的求和**，或是**ζ的平方和，**于是我们在原来的优化问题的基础上加上惩罚项

![clip_image002[13]](/assets/img/ML/one-stop-machine-learning/soft-margin-full.gif)

公式解析：

- 参数C描述我们有多在乎松弛项，C越大，outlier带来的影响越大，反之越小。如果C无限大，只要有一个样本不能正确分类，目标函数就会无限大，整个优化问题变成“硬间隔”分类。
- 并不是所有的点都有松弛项的，对于能够正确分类的样本，松弛量为0，对于不能正确分类的样本，才有惩罚项

**松弛变量和核函数都是用来解决线性不可分问题的，这两者有什么不同？：**

- 实际问题中，原始特征空间通常高度线性不可分，先通过核函数的形式，隐性把原始特征向量映射到高维向量，但是一般高维向量也不是完全线性可分的，是近线性可分的。、
- SVM简单来说就是**使用核函数的软间隔线性分类器**

**松弛项的改进方案：**

首先我们可以对不同的离群点采用不同的C值，对于那些绝对不能分类错的点采用很大的C值，有些不重要的点就采用很小的C值

其次，如果样本集本身的类别“偏斜”，即类别不平衡问题，一个类别样本的数量比另一个类别的样本数大的多的情况。这种情况下，minority category的样本的边界值极有可能没有达到真实边界值，分类间隔往往比理想的间隔要大，如图虚线方框是真实样本，但是在训练的时候，由于样本量有限，并么有出现该边界样本。

![image_2](/assets/img/ML/one-stop-machine-learning/relaxation.png)

这种情况下，可以对不同类别的样本采用不同的C值，少数类别样本具有更大的C值，本来就是少数类别，他们的分类正确性就应该更加重视，惩罚项优化为如下：

![clip_image002[5]](/assets/img/ML/one-stop-machine-learning/soft-margin-regulator.gif)

SVM 特点：

- 小样本
- 非线性，通过松弛变量 和 核函数 实现
- 高维模型识别
- 优秀的泛化能力，这是是因为其本身的优化目标是结构化风险最小，而不是经验风险最小，因此，通过margin的概念，得到对数据分布的结构化描述，因此减低了对数据规模和数据分布的要求
- 它是一个凸优化问题，因此局部最优解一定是全局最优解的优点
- 泛化错误率低，分类速度快，结果易解释

**缺点：**

1. 大规模训练难以实现

   SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。针对以上问题的主要改进有有J.Platt的SMO算法、T.Joachims的SVM、C.J.C.Burges等的PCGC、张学工的CSVM以及O.L.Mangasarian等的SOR算法。

   如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用了简单的naive bayes分类器，或者是使用逻辑回归模型分类

2. 多分类问题存在困难

3. 对核函数和丢失数据敏感

**Reference**

[SVM原理详解](https://blog.csdn.net/abcd_d_/article/details/45094473)

[支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)

## Linear Regression

cost Function：

![img](/assets/img/ML/one-stop-machine-learning/LR-cost-func.png)

cost function即为所有样本的均方误差

为了求解权重theta，使用梯度下降法

![img](/assets/img/ML/one-stop-machine-learning/LR-theta.png)

**Reference**

[机器学习入门：线性回归及梯度下降](https://blog.csdn.net/xiazdong/article/details/7950084)

## Logistics Regression

线性分类模型在SVM部分已经介绍了这里简单回忆下：

**线性函数：**
$$
y = w^Tx + b
$$

函数输出是连续值，用于回归问题，怎么把回归问题转换为分类问题呢。SVM采用的是把y>0 的归类为类别1，y<0的归类于类别2。LR模型采用的**sigmoid函数，线性函数结合sigmoid就成了LR模型**

$$
y = \sigma(f(x)) = \sigma(w^Tx) = \frac{1}{1+e^{-w^Tx}}
$$

注意，**这里的w^T，x 都是已经包含了偏置项的，只需要在原来的基础上变成x_new = [x, 1] ,  w_new = [w，b]**

一个样本，它类别为1 的概率为：

$$
P_{y=1} = \frac{1}{1+e^{-w^Tx}}=p
$$

那么类别为0的概率

$$
P_{y=0} = 1-p
$$

一个事件发生也不发生的比例：

$$
\frac{P(y=1|x)}{P(y=0|x)} = \frac{p}{1-p} = e^{g(x)}
$$

称为事件发生比；记为odds
写作：

$$
P(y|x) = 
\begin{cases}
	p, \text{y=1}\\
    1-p, \text{y=0}
\end{cases}
$$

统一一下形式，转为一个公式表示就是：

$$
p(y_i|x_i) = p^{y_i}(1-p)^{1-y_i} \rightarrow 式1
$$

假设一组样本集有N个样本，那么这个样本集发生的概率就是单个样本发生概率的相乘，称为**联合分布，也是式1的\*似然函数\***：

$$
P总 = P(y1|x1)P(y2|x2)P(y3|x3)...P(yN|xN) = \prod_{i=1}^{N}p^{y_n}(1-p)^{1-y_n} \rightarrow 式2
$$

连乘形式不方便计算，两边同时取对数：

$$
\begin{equation*}
\begin{split}
F(w) = & ln(P总)\\
&= ln(\prod_{n=1}^{N}p^{y_n}(1-p)^{1-y_n})\\
&=\sum_{n=1}^{N}ln(p^{y_n}(1-p)^{1-y_n})\\
&=\sum_{n=1}^{N}(y_nln(p)+(1-y_n)ln(1-p)) \text 式3
\end{split}
\end{equation*}
$$

此时的**F(x)是LR模型的损失函数**，当然有的博客中的F(x)也可以除以N，不影响结果，但是为什么F(x)就是损失函数呢？

**这是由极大似然法推导而来**

**极大似然法**

如果用极大似然法求解式3，上式中只有p中带有一个w是未知量。**我们现在要做的就是找到一组合适的W使得样本集的联合分布概率最大**。解释来说就是，样本集这个事件已经真实发生了，我们倒过去，我们要找一组权重，尽可能的使得它发生的概率最大，这样，这一组w就尽可能的接近真实w。例如，已知一个学校男生比女生7:3 ，一个是3:7，现在抽一个人是男生，那么最可能来自哪个学校？当然是第一个学校，虽然可能出错，不过已经是已知条件下概率最大的结果了。

首先对权重n+1（因为包含了偏置b）分别求偏导，假设对wk求偏导：

![20140528205737500](/assets/img/ML/one-stop-machine-learning/max-likely.jpg)
![20140528210021312](/assets/img/ML/one-stop-machine-learning/max-likely2.jpg)

产生n+1 个等式，然后利用**牛顿法**迭代求解；

到现在为止我们还是没有解释为什么式3就是损失函数，直到我们不用极大似然估计和牛顿法去求解而是用梯度上升/下降法去求解，就很明显了。

**梯度上升法/下降法**

我们同时也可以用**梯度上升法**求解式3，求得一组w使得式3的达到最大值。或者我们在式3前面乘上一个负因子

![gradient-asc](/assets/img/ML/one-stop-machine-learning/gradient-asc.jpg)

那么梯度上升法，就转换为了梯度下降法。**此时问题就变成了寻找一组w使得因子加权后的似然函数最小，是不是很熟悉，这个因子加权后的似然函数，不就是机器学习中常见的损失函数嘛**

![20131113203723187](/assets/img/ML/one-stop-machine-learning/LR-loss-solu.jpg)

其中 g(x) = θT * x, 并且

![20131113203741453](/assets/img/ML/one-stop-machine-learning/LR-loss-solu2.jpg)

权重θ 更新函数：由于1/m是个常数，和学习率a合并

![20131113205240203](/assets/img/ML/one-stop-machine-learning/LR-loss-func3.jpg)

**梯度下降过程向量化**

见公式15，每次梯度更新都需要遍历m个样本，但是在实际应用中，往往不用for loop实现，而是用vector 内积实现for循环的效果

记θ*x 即样本输入点乘权重的结果，经过sigmoid函数之前

![20131113204012546](/assets/img/ML/one-stop-machine-learning/LR-gd.jpg)

基于向量A计算误差向量E：

![20131113204103593](/assets/img/ML/one-stop-machine-learning/LR-gd2.jpg)

有了误差向量E之后， 权重更新就可以实现向量化，省去for loop， θj可以表示为式23，整个权重θ统一表示为式24

![20131113204138093](/assets/img/ML/one-stop-machine-learning/LR-gd3.jpg)
![20131113204152062](/assets/img/ML/one-stop-machine-learning/LR-gd4.jpg)

**逻辑回归为什么要用sigmoid函数？能不能用其他函数？**

**为什么是sigmoid？**

首先想到是用W*X来表示属于类别1的概率，因为w*x的值越大离分类面越远，有两个问题

1. 这个值是负无穷到正无穷，需要归一化为0-1
2. 现实中，w*x 非常大或者非常小的时候对概率值影响不大，但是w*x 在0范围附近，也就是在分类面的附近对概率影响愈来愈大。离分类面很远，再远一点也对概率值理论上影响不大

于是需要修正，首先想到的思路是时间的几率odds = p/(1-p)是0到正无穷，同时为了满足第二个条件，对odds取log 就同时满足了2个条件

$$
log(\frac{p}{1-p}) = w^Tx \Longrightarrow p=\frac{1}{1+e^{-w^Tx}}
$$

**为什么只能是sigmoid？**

1.正态分布解释

我们不知道分类事件的发生符合什么分布形式，一般默认符合正态分布，正态分布的概率密度函数是钟形，但是其分布函数却是类似“sigmoid”的形状，而且sigmoid的求导方便又很近似正态分布，所以采用sigmoid

2.最大熵解释

该解释是说，在我们给定了某些假设之后，我们希望在给定假设前提下，分布尽可能的均匀。对于Logistic Regression，我们假设了对于{X,Y}，我们预测的目标是Y|X，并假设认为Y|X服从bernoulli distribution，所以我们只需要知道P(Y|X)；其次我们需要一个线性模型，所以P(Y|X)=f(wx)。接下来我们就只需要知道f是什么就行了。而我们可以通过最大熵原则推出的这个f，就是sigmoid

无论是sigmoid函数还是probit函数都是**广义线性模型的连接函数**（link function）中的一种。选用联接函数是因为，从统计学角度而言，**普通线性回归模型是基于响应变量和误差项均服从正态分布的假设，且误差项具有零均值，同方差的特性**。但是，例如分类任务（判断肿瘤是否为良性、判断邮件是否为垃圾邮件），其响应变量一般不服从于正态分布，其服从于二项分布，所以选用普通线性回归模型来拟合是不准确的，因为不符合假设，所以，我们需要选用广义线性模型来拟合数据，通过标准联接函数(canonical link or standard link function)来映射响应变量，如：正态分布对应于恒等式，泊松分布对应于自然对数函数，二项分布对应于logit函数（二项分布是特殊的泊松分布）。

**Reference**<br>[Logistic回归原理及公式推导](https://blog.csdn.net/AriesSurfer/article/details/41310525)<br>[机器学习--Logistic回归计算过程的推导](https://blog.csdn.net/ligang_csdn/article/details/53838743)<br>[逻辑斯蒂回归原理篇](https://blog.csdn.net/a819825294/article/details/51172466 )<br>[sigmoid函数与softmax函数](https://www.jianshu.com/p/52fcd56f2406)<br>[逻辑回归(logistics regression)](https://blog.csdn.net/weixin_39445556/article/details/83930186)

## KNN

KNN是一种**基于instance的学习算法**，基于instance的学习方法只是简单的把训练样本存储起来。每当学习器遇到一个新的instance，分析新的实例和以前存储实例的关系，据此把一个目标函数值赋给新实例

基于实例方法的不足

- 分类时的开销大，所有的计算基本都发生在分类的时候
- 当从存储器中检索相似的训练样例时，它们一般考虑实例的所有属性。如果目标概念仅依赖于很多属性中的几个时，那么真正最“相似”的实例之间很可能相距甚远。

KNN假设实例是n维空间中的一个点，点与点的距离是由标准欧式距离定义的,当然也可以用Lp距离来定义，在新样本输入，对已知的每个样本点计算距离，取前k个样本中个数最多的类别作为新样本的类别，也可以进行加权求和做为新样本的类别或者回归问题的输出。改进型有weighted knn，根据距离进行贡献加权，很好理解。

**KNN算法实现，KD树**

![WeChat Screenshot_20190811181452](/assets/img/ML/one-stop-machine-learning/knn-kd.png)

![WeChat Screenshot_20190811181503](/assets/img/ML/one-stop-machine-learning/knn-kd2.png)

有几个重要的点，划分维度是循环的，假设有2个特征维度，6个数据点，kd树大概3层，第一层按照x轴划分，左右子集分别按照y轴划分，他们的左右子集又按照x轴划分，中位数所在的那个样本留在根节点，不被分为左右样本，这样每个节点至少有一个样本、

搜索kd树的算法

![081118282872_0WeChat Screenshot_20190811182745](/assets/img/ML/one-stop-machine-learning/kd-search.png)

在建树的过程中， 每个叶子节点相当于分配了一个区域

首先找到所在叶子节点，按照输入节点和所在叶子节点画球，往上遍历到父节点，如果父节点的另一个子节点的区域和这个球相交，说明可能这个区域内有更近的，遍历整个区域的点，如果有，更新最近点，然后再按照输入节点和最近的点画球。直到访问到最上面的根节点

**特点**：

- 对样本量大的类别有偏向，但是在样本量足够大的情况下，效果不错。由于是k个加权平均，robustness 强
- 距离容易被不相关的属性支配。距离是基于所有属性计算的，有些feature对于类别的重要性比较大，可能就在计算中导致偏差较大，可以考虑对属性进行加权
- k值的选择对结果的影响很大，如果k值越小，相当于用越少的邻域中的训练实例进行预测，只有很相似的样本点参与预测，近似误差小，当前于在测试集上表现良好，模型越复杂，但是估计误差会增大，容易受噪声点支配；k越大，相当于在越大的邻域中进行预测，一些不相关的点也进来预测，近似误差大，但是估计误差好一点，模型越简单，极限情况下，k=N，模型一直返回多数类，相当于过于简单，返回恒定值

**Reference**<br>[数据挖掘十大算法--K近邻算法](https://wizardforcel.gitbooks.io/dm-algo-top10/content/knn.html)

## NaiveBayesian

朴素贝叶斯属于生成式模型，学习输入和输出的联合概率分布。给定输入x，利用贝叶斯概率定理求出最大的后验概率作为输出y。首先肯定是贝叶斯公式, 其中P(B\|A)称为后验概率 P（A\|B）称为似然函数，P(A), P(B)为先验概率.

$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$

朴素贝叶斯的思想很简单：给出一个待分类项，出现这个待分类项的情况下，各个类别出现的概率，那个高就取那个类别作为分类结果；就和看到一个黑人，预测他的国家，假设我们现在数据集中，70%黑人是非洲，20%美洲，10%欧洲，那肯定预测非洲更加准确。

朴素贝叶斯之所以叫naive bayes，是因为它**假设了x的各个属性之间条件独立**，这个假设很胆大，在很多情况下是不成立的，所有朴素贝叶斯的效果很多时间都不太好。换言之，该假定说明给定实例的目标值情况下，观察到联合的$a_1a_2a_3a_4...a_n$的概率正好是对每个单独属性的概率乘积：

$$
P(a_1,a_2,...,a_n|b_j) = \prod_iP(a_i|b_j)
$$

**朴素贝叶斯分类的正式定义如下：**

1. 设$x=\{a_1,a_2,a_3,...,a_n\}$为一个待分类项，而每个a为x的一个特征属性

2. 有类别集合$C = \{b_1,b_2,b_3,...,b_n\}$

3. 计算P(b1\|x),P(b2\|x),...,P(bn\|x)​

4. 如果P(bk\|x) = max{P(b1\|x),P(b2\|x),...,P(bn\|x)}则$x\in b_k$

那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：

1. 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。
2. 统计得到在各类别下各个特征属性的条件概率估计即

$$
P(a_1|b_1),...,P(a_m|b_1);P(a_1|b_2),...,P(a_m|b_2);...
$$

3. 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：

$$
P(b_i|x) = \frac{P(x|b_i)P(b_i)}{P(x)}
$$

因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：

$$
P(x|b_i)P(b_i) = P(b_i)\prod_{j=1}^{m}P(a_j|b_j)
$$

但是仅仅做到以上步骤并不够，**如果预测样本中出现了训练样本中没有出现过的值**，那么所有的后验概率都是0，预测过程就失去了意义。常见的三种模型是多项式模型，高斯模型和伯努利模型，从不同的角度解决了0概率问题。

**多项式模型**<br>适用于**离散型数据**，多项式模型在处理先验概率和似然概率的时候分别进行了平滑处理，分别如下

$$
P(y_i) = \frac{n_{y_k}+a}{n+ma} \quad n为总样本数,m为类别总数,a为平滑值\\
P(x_i|y_k) = \frac{n_{y_kx_i}+a}{n_{y_k}+na} \quad n是特征可能的取值个数
$$

当$a=1$ 称作Laplace平滑，当$0<a<1$ 时称为Lidstone平滑，当$a=0$ 不作任何处理

**高斯模型**<br>适用于**连续性变量**，如果把连续性特征当成离散处理，那么预测时很多似然概率都会为0，所以假设连续性变量每一个维度遵循正态分布，似然概率计算公式

$$
P(x_i|y_k) = \frac{1}{\sqrt{2\pi\sigma_{yk,i}^2}}e^-{\frac{(x_i-\mu_{y_k,i}^2)}{2\sigma_{yk,i}^2}}
$$

**伯努利模型**<br>适用于**离散型数据**，但和多项式模型不同的是，伯努利模型中每个特征的取值只能是0或者1(例如某种数据进行onehot之后的结果)，似然概率公式

$$
P(x_i|y_k) = P(x_i=1|y_k)\quad 当x_i=1\\
P(x_i|y_k) = 1-P(x_i=1|y_k)\quad 当x_i=0
$$

**Reference**<br>[朴素贝叶斯分类器](https://wizardforcel.gitbooks.io/dm-algo-top10/content/naive-bayes.html)

## Model Comparison

### 高维稀疏特征

结论：GBDT或者说其他树模型(个人觉得XGBoost例外，后续详细说明)对于高维稀疏特征很容易过拟合而且不适合处理，而带惩罚项的LR对稀疏特征不容易过拟合。

原因：**不适合高维稀疏特征的原因**：one-hot特征极大扩展了特征数量，可供划分的特征多了，训练时长变长； **容易过拟合的原因**：如果某个维度的特征取1的样本正好和label为1的样本高度重叠，那么树模型只需要一个节点就可以很好的划分数据，对于一般用叶子结点个数和深度做惩罚项的树模型来说，几乎没有惩罚，反而是很鼓励这种情况，但xgboost以叶子结点的输出值L2范数为惩罚，会惩罚取值大的样本聚集的情况，假设label不是0,1而是0和100，如果取值100的样本高度聚合，惩罚项较大，惩罚这种极端情况。另外，如果样本分布不平衡，GBDT会专注学习少数样本的特征，找到那些能很好划分少数样本的规律，导致过拟合。反观带L1,L2正则项的线性模型，如果一个特征的权重特别大，L2正则项起到惩罚作用，遇到样本不平衡的问题，惩罚项也可以使得线性模型不至于全部学习少数样本的特征。

**Reference**<br>[LR,GBDT高维稀疏特征](https://blog.csdn.net/bit_max/article/details/101230299)<br>[高维稀疏特征的时候，lr 的效果会比 gbdt 好](https://blog.csdn.net/papaaa/article/details/79910449)

### LR和SVM的异同

- LR是参数模型，SVM是非参数模型
- Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别处于极其不平衡的状态, 一般需要先对数据做平衡处理。
- Linear SVM依赖数据表达的距离测度，所以需要对数据先做标准化；LR不受其影响
- Linear SVM依赖惩罚项的系数，实验中需要做[交叉验证](https://zhuanlan.zhihu.com/p/32627500)
- Linear SVM和LR的执行都会受到异常值的影响，其敏感程度而言，谁更好很难下明确结论。
- Linear SVM和LR损失函数不同, LR为logloss, SVM为hinge loss. 而SVM中的
  称为**hinge loss**。
- LR 能做的 SVM都能做，但可能在准确率上有问题，SVM能做的LR有的做不了。

### LR和线性回归的区别

线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

### GBDT和随机森林的异同

相同

- 都是由多棵树组成
- 最终的结果都是由多棵树一起决定

不同

- 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成；

- 组成随机森林的树可以并行生成；而GBDT只能是串行生成；

- 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来；

- 随机森林对异常值不敏感，GBDT对异常值非常敏感；

- 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成；

- 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能。

### GBDT 和XGBoost的不同

- GBDT的梯度拟合只考虑了一阶，XGBoost做了泰勒2阶展开

- XGBoost损失函数加入正则项，正则函数系数 *节点个数+系数*（叶子节点值的L2范数和）

- XGBoost支持并行化

### XGBoost, LR, RF对比

XGBoost：

优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，XGBoost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）XGBoost还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

适用场景：分类回归问题都可以。

RF：

优点：1）表现性能好，与其他算法相比有着很大优势。2）随机森林能处理很高维度的数据（也就是很多特征的数据），并且不用做特征选择。3）在训练完之后，随机森林能给出哪些特征比较重要。4）训练速度快，容易做成并行化方法(训练时，树与树之间是相互独立的)。5）在训练过程中，能够检测到feature之间的影响。6）对于不平衡数据集来说，随机森林可以平衡误差。当存在分类不平衡的情况时，随机森林能提供平衡数据集误差的有效方法。7）如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。8）随机森林算法有很强的抗干扰能力（具体体现在6,7点）。所以当数据存在大量的数据缺失，用RF也是不错的。9）随机森林抗过拟合能力比较强（虽然理论上说随机森林不会产生过拟合现象，但是在现实中噪声是不能忽略的，增加树虽然能够减小过拟合，但没有办法完全消除过拟合，无论怎么增加树都不行，再说树的数目也不可能无限增加的）。10）随机森林能够解决分类与回归两种类型的问题，并在这两方面都有相当好的估计表现。（虽然RF能做回归问题，但通常都用RF来解决分类问题）。11）在创建随机森林时候，对generlization error(泛化误差)使用的是无偏估计模型，泛化能力强。

缺点：1）随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。2）对于许多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试。3）可能有很多相似的决策树，掩盖了真实的结果。4）对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。5）执行数据虽然比boosting等快（随机森林属于bagging），但比单只决策树慢多了。

适用场景：数据维度相对低（几十维），同时对准确性有较高要求时。因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

LR：

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

